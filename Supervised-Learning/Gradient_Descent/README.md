# Gradient Descent - Optimization and Regression


This notebook demonstrates **Gradient Descent** as a foundational optimization algorithm for supervised learning.  
Using a regression setting, the notebook applies gradient descent to estimate model parameters by iteratively minimizing the mean squared error between predicted and actual values.

Unlike closed-form solutions, gradient descent provides a flexible, scalable approach that generalizes to high-dimensional or non-analytical models.  
This implementation shows how step size, convergence behavior, and loss evolution shape the learning dynamics of regression models.